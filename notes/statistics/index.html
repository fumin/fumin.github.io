<html>
  <head>
    <meta charset="utf-8"/>
    <script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  </head>
  <style>
body {
  font-size: xx-large;
}
table {
  font-size: xx-large;
}
  </style>
  <body>
    <div id="container" style="margin-left:auto; margin-right:auto;">
<h1>Statistics</h1>

<h2>Mean \(E(X)\) may not capture a problem's concern</h2>
<p>
<q>
Flip a fair coin 100 times—it gives a sequence of heads (H) and tails (T). For each HH in the sequence of flips, Alice gets a point; for each HT, Bob does, so e.g. for the sequence THHHT Alice gets 2 points and Bob gets 1 point. Who is most likely to win?
</q>
-- <a href="https://twitter.com/littmath/status/1769044719034647001">Daniel Litt</a></p>
<p>Answer: Bob wins</p>
<p>An intuitive explanation is that HH's distribution is more skewed due to the fact that HH's possible scores are much higher due to consecutive heads. Thus, HH's distribution has more probability mass in the region <code>x &lt; mean</code>. Since HH and HT have the same mean, it follows that <code>HH &lt; HT</code> happens more frequently.</p>
<p>Let <code>n</code> be the sequence length, the mean of Alice's and Bob's distributions are the same <code>(n-1)/4</code>. The below diagram shows the distributions of <code>HH</code> and <code>HT</code> and <code>HH - HT</code>:</p>
<div style="text-align: center;"><img src="n16.png"></div>
<div style="text-align: center;"><img src="n16diff.png"></div>
<p>As we increase n, the number of possible sequences increases exponentially. Thus we experiment with the sampling method, too. As shown in the below diagrams for <code>n = 32</code>, the sampling method matches pretty well with the exact one:</p>
<div style="text-align: center;"><img src="n32.png"></div>
<div style="text-align: center;"><img src="n32diff.png"></div>
<p>Having confirmed that sampling works, we plot the case for <code>n = 1024</code></p>
<div style="text-align: center;"><img src="n1024.png"></div>
<p>It seems that the Central Limit Theorem is in effect here. Moreover the edge HT over HH has diminishes as the sequence length increases:</p>
<table style="margin-left: auto; margin-right: auto; border-spacing: 1em 0;">
  <thead><tr>
    <td></td>
    <td>16</td>
    <td>32</td>
    <td>1024</td>
  </tr></thead>
  <tbody>
    <tr><td><code>hh &lt; ht</code></td><td>0.464</td><td>0.475</td><td>0.495</td></tr>
    <tr><td><code>hh == ht</code></td><td>0.143</td><td>0.100</td><td>0.018</td></tr>
    <tr><td><code>hh &gt; ht</code></td><td>0.393</td><td>0.425</td><td>0.487</td></tr>
  </tbody>
</table>

<h2>Mean Deviation is more useful than Standard Deviation</h2>
<p>The classical argument regarding the Standard Deviation \(\sqrt{\frac{1}{n-1}\sum(x-\overline{x})^{2}}\) having an advantage over the Mean Absolute Deviation \(\frac{1}{n}\sum\left| x - \overline{x} \right|\) is a fragile one.
It works only for the perfect Normal distribution and collapses at the slightest occurance of fat tails.
In mathematical terms, the argument states that the statistical efficiency \(\frac{Var(X)}{E(X)^{2}}\) of the Standard Deviation is better than that of the Mean Deviation:</p>
$$\displaylines{
\begin{aligned}
\text{Asymptotic Relative Efficiency (ARE)} =\space & \underset{n\to \infty }{lim}\left( \frac{\frac{Var(Std)}{E(Std)^{2}}}{\frac{Var(Mad)}{E(Mad)^{2}}} \right) \\
=\space & \underset{n\to \infty }{lim}\frac{\frac{n{\Gamma(\frac{n}{2})}^{2}}{2{\Gamma(\frac{n+1}{2})}^{2}}-1}{\frac{\pi-2}{2n}} \\
=\space & \frac{1}{\pi-2} \simeq 0.875
\end{aligned}
}$$
<p>However, this result quickly becomes invalidated when considering the simplest model for volatility, which is a mixing model with an occasional jump with probability \(p\):</p>
$$\displaylines{
Var(x)=\begin{cases}
\sigma^{2}(1+a) & \text{with probability $p$} \\
\sigma^{2} & \text{with probability $p-1$}
\end{cases}
}$$
<p>The below diagram shows simulation results with \(p=0.01\) and \(n=10000\):</p>
<div style="text-align: center;"><img src="are.png"></div>
<p>We see that even though for \(a=0\) \(RE=0.878\), for \(a=2\) \(RE=1.434\) which starts to cause degradation. In other words, a minute presence of outliers makes MAD more efficient than STD.</p>
<p>Morever, many statistical phenomena and processes have "infinite variance" (such as the popular Pareto 80/20 rule) but have finite, and sometimes very well behaved, mean deviations.
Whenever the mean exists, MAD exists. The reverse (infinite MAD and finite STD) is never true.</p>
<p>In summary, we should retire the notion of Standard Deviation and replace it with the more effective one of Mean Deviation.
Standard deviation, STD, should be left to mathematicians, physicists and mathematical statisticians deriving limit theorems.
There is no scientific reason to use it in statistical investigations in the age of the computer.</p>

<h2>The Central Limit Theorem does not apply to many real world problems with finite sample sizes</h2>
<p>The Generalized Central Limit Theorem states that the sum of random variables \(S_{n} = X_{1}+X_{2}+\cdots +X_{n}\) converges to \(n\mu + n^{\frac{1}{\alpha}}\text{Stable}(\alpha)\) where \(\text{Stable}(\alpha)\) is the <a href="https://en.wikipedia.org/wiki/Stable_distribution">Stable distribution</a>, parameterized by the tail index \(\alpha\).
Thus when \(\alpha > 1\), \(\underset{n\to \infty}{lim}\left( \frac{S_{n}}{n} - \mu \right) = n^{\frac{1}{\alpha}-1}  = 0\) and we get the Law of Large Numbers.
The below graph (Fig 8.1<sup id="cite_ref-1a"><a href="#cite_note-1">[1a]</a></sup>) illustrates this phenomenon assuming \(X_{n}\) has been subtracted by the linear trend \(\mu\) leaving the random walk \(S_{n}\):
<div style="text-align: center;"><img src="kappa.png"></img></div>
A consequence of this is that for all powers \(p=1,2,3,\cdots\), if \(E(X^{p}) < \infty\) then (10.2.6<sup id="cite_ref-1b"><a href="#cite_note-1">[1b]</a></sup>)
$$\displaylines{
\underset{n\to \infty}{lim}\frac{\text{max}(X_{1}^{p},X_{2}^{p},\cdots ,X_{n}^{p})}{X_{1}^{p}+X_{2}^{p}+\cdots +X_{n}^{p}} = 0
}$$
For example, in the case of \(p=4\), the existence of the 4th moment should prevent a single data point to dominate the kurtosis's calculation as \(n\) increases.
This behaviour is observed for the Gaussian distribution, but not for both the Taiwan<sup id="foot_ref-1"><a href="#foot_note-1">1</a></sup> and U.S. stock markets, as shown in the below Max-to-Sum plot (MS plot):
<div style="text-align: center; position: relative; left: -10vw; width: 90vw;"><img src="ms_taiex.png" style="width: 50%;"/><img src="ms_taiex_gauss.png" style="width: 50%;"/></div>
<div style="text-align: center;"><img src="ms_sp500.png"/></div>
<table style="margin-left: auto; margin-right: auto; margin-top: 1em; border-spacing: 1em 0;">
  <thead><tr>
    <td></td>
    <td>dataset size</td>
    <td>Max-to-Sum</td>
  </tr></thead>
  <tbody>
    <tr><td>Taiwan stock index (TAIEX)</td><td>777 (58 years)</td><td>0.16</td></tr>
    <tr><td>S&amp;P 500</td><td>867 (69 years)</td><td>0.24</td></tr>
    <tr><td>Gaussian with same σ as TAIEX</td><td>777</td><td>0.04</td></tr>
    <tr><td colspan="3" style="text-align: center;">Data below from Table 7.2 Taleb<sup id="cite_ref-1c"><a href="#cite_note-1">[1c]</a></sup></td></tr>
    <tr><td>Silver</td><td>46 years</td><td>0.94</td></tr>
    <tr><td>Crude Oil</td><td>26 years</td><td>0.79</td></tr>
    <tr><td>Yen/USD</td><td>38 years</td><td>0.27</td></tr>
    <tr><td>Nasdaq index</td><td>21 years</td><td>0.13</td></tr>
    <tr><td>Australian Dollar/USD</td><td>22 years</td><td>0.12</td></tr>
    <tr><td>Bonds 30Y</td><td>32 years</td><td>0.02</td></tr>
  </tbody>
</table>
</p>
<p>
The reason for the volality in the kurtosis of both the Taiwan and U.S. stock markets is due to their fat tails.
To quantify how fat the tails are, we fit them to the pareto distribution \(\frac{1}{x^{\alpha + 1}}\) where alpha is the tail index.
The smaller \(\alpha\), the fatter the tail; for \(\alpha <= 4\) kertosis does not exist; for \(\alpha <= 3\) skewness does not exist; for \(\alpha <= 2\) variance becomes infinite; for \(\alpha <= 1\) mean becomes infinite.
<div style="text-align: center;"><img src="loglog_taiex.png"></img></div>
<div style="text-align: center;"><img src="loglog_sp500.png"></img></div>
We see that the fitted tail indices are smaller than 3, which explains why the 3rd and 4th moments in the MS plots never converge.
Without the 4th moment, the sum of squares converges to \(n\text{Stable}(\alpha)\) with \(\alpha < 1\) instead of the <a href="https://en.wikipedia.org/wiki/Variance#Distribution_of_the_sample_variance">chi-square distribution</a>.
In this case, the expected value of the variance \(E\left( \frac{n\text{Stable}(\alpha)}{n} \right) = E(\text{Stable}(\alpha)) = \infty\).
Moreover, since the chi-square test for variance and the <a href="https://en.wikipedia.org/wiki/F-test#Regression_problems">F-test for regression</a> depend on the convergence to the chi-square distribution, they cease to work for the Taiwan and U.S. stock markets.
To use similar conventional statistical tests that rely on the gaussian distribution, we need precise measurements on the differences between the true distribution and the gaussian under the same sample size.
</p>
<p>
One such quantitative metric to measure the convergence speed of the Central Limit Theorem is the \(\kappa\) metric introduced by Nassim Taleb (Equation 8.1 <sup id="cite_ref-1d"><a href="#cite_note-1">[1d]</a></sup>).
Let \(\mathbb{M}(n) = \mathbb{E}(\left| S_{n} - \mathbb{E}(S_{n}) \right|)\) be the mean absolute deviation for \(n\) summands, define
$$\displaylines{
\kappa(n_{0}, n) = 2 - \frac{\text{log}(\frac{n}{n_{0}})}{\text{log}(\frac{\text{min}\left\{ \mathbb{M}(i),\space i\ge n \right\} }{\mathbb{M}(n_{0})})}
}$$
By definition, \(\kappa(n_{0}, n) \ge \kappa(n_{0}, n+1)\), and for stable distributions \(\kappa(n_{0}, n) = 2 - \alpha\).
Since the Gaussian distribution corresponds to \(Stable(2)\), \(\kappa_{gaussian} = 0\).
</p>
<p>
Let \(P\) be some distribution, \(n_{g}\) be some sample size, since \(\kappa\) is related to the mean absolute deviation \(\mathbb{M}\), we can use it to calculate the minimum sample size \(n_{\nu}\) such that the deviation of said distribution \(P\) \(\frac{\mathbb{M}_{P}(n_{nu})}{n_{nu}}\) is less than that of a Gaussian \(\frac{\mathbb{M}_{gaussian}(n_{g})}{n_{g}}\).
Let \(n_{0}=1\), and scale \(P\) such that \(\mathbb{M}_{P}(1)=\mathbb{M}_{gaussian}(1)\), then
$$\displaylines{
\begin{aligned}
\frac{\mathbb{M}_{P}(n_{\nu})}{n_{\nu}} < \frac{\mathbb{M}_{gaussian}(n_{g})}{n_{g}} & \Rightarrow \frac{n_{\nu}^{\frac{1}{2-\kappa(n_{\nu})}}}{n_{\nu}} < \frac{n_{g}^{\frac{1}{2}}}{n_{g}} \\
& \Rightarrow n_{\nu} < n_{g}^{\frac{2-\kappa(n_{\nu})}{2(1-\kappa(n_{\nu}))}} \\
\end{aligned}
}$$
Since \(n_{\nu} > n_{g}\), thus \(\kappa(n_{\nu}) < \kappa(n_{g})\). 
Note also for distributions where the mean exists, that is with a tail index larger than 1, \(0 \le \kappa(n_{g}) \le 1\), and thus \(\kappa(n_{g}) < \frac{2\kappa(n_{g})}{\kappa(n_{g})+1}\), and \(\kappa(n_{\nu}) < \frac{2\kappa(n_{g})}{\kappa(n_{g})+1}\). In this case,
$$\displaylines{
\begin{aligned}
\kappa(n_{\nu}) < \frac{2\kappa(n_{g})}{\kappa(n_{g})+1} & \Rightarrow \frac{2-\kappa(n_{\nu})}{2(1-\kappa(n_{\nu}))} < \frac{1}{1-\kappa(n_{g})} \\
& \Rightarrow n_{\nu} < n_{g}^{\frac{1}{1-\kappa(n_{g})}}
\end{aligned}
}$$ 
In other words, given \(n_{g}\), we can use \(n_{\nu} = n_{g}^{\frac{1}{1-\kappa(n_{g})}}\) to gaurantee that \(\frac{\mathbb{M}_{P}(n_{\nu})}{n_{\nu}} < \frac{\mathbb{M}_{gaussian}(n_{g})}{n_{g}}\).
For the pareto distribution, we have the following values for \(\kappa(n)\):
</p>
<p>
<table style="margin-left: auto; margin-right: auto; border-spacing: 1em 0;">
  <caption id="table-1">Table 1: \(\kappa_{\alpha}(n)\) values for the Pareto distribution</caption>
  <thead><tr>
    <td>\(\alpha\)</td>
    <td>2</td>
    <td>30</td>
    <td>100</td>
    <td>1,000</td>
    <td>10,000</td>
    <td>100,000</td>
    <td>1,000,000</td>
    <td>∞</td>
  </tr></thead>
  <tbody>
    <tr><td>1.16</td><td>0.879</td><td>0.863</td><td>0.858</td><td>0.853</td><td>0.850</td><td>0.848</td><td>0.847</td><td>0.840</td></tr>
    <tr><td>1.25</td><td>0.829</td><td>0.799</td><td>0.790</td><td>0.779</td><td>0.772</td><td>0.768</td><td>0.765</td><td>0.75</td></tr>
    <tr><td>1.50</td><td>0.724</td><td>0.660</td><td>0.639</td><td>0.608</td><td>0.587</td><td>0.572</td><td>0.561</td><td>0.50</td></tr>
    <tr><td>1.75</td><td>0.650</td><td>0.560</td><td>0.529</td><td>0.482</td><td>0.447</td><td>0.420</td><td>0.400</td><td>0.25</td></tr>
    <tr><td>2.00</td><td>0.594</td><td>0.487</td><td>0.449</td><td>0.391</td><td>0.348</td><td>0.314</td><td>0.286</td><td>0</td></tr>
    <tr><td>2.25</td><td>0.551</td><td>0.431</td><td>0.389</td><td>0.326</td><td>0.279</td><td>0.242</td><td>0.214</td><td>0</td></tr>
    <tr><td>2.50</td><td>0.517</td><td>0.387</td><td>0.343</td><td>0.278</td><td>0.231</td><td>0.195</td><td>0.169</td><td>0</td></tr>
    <tr><td>2.75</td><td>0.488</td><td>0.353</td><td>0.308</td><td>0.242</td><td>0.197</td><td>0.164</td><td>0.140</td><td>0</td></tr>
    <tr><td>3.00</td><td>0.465</td><td>0.325</td><td>0.280</td><td>0.215</td><td>0.172</td><td>0.142</td><td>0.120</td><td>0</td></tr>
    <tr><td>3.25</td><td>0.445</td><td>0.302</td><td>0.257</td><td>0.194</td><td>0.153</td><td>0.125</td><td>0.106</td><td>0</td></tr>
    <tr><td>3.50</td><td>0.428</td><td>0.283</td><td>0.238</td><td>0.178</td><td>0.139</td><td>0.113</td><td>0.095</td><td>0</td></tr>
    <tr><td>3.75</td><td>0.413</td><td>0.267</td><td>0.223</td><td>0.164</td><td>0.128</td><td>0.104</td><td>0.087</td><td>0</td></tr>
    <tr><td>4.00</td><td>0.400</td><td>0.253</td><td>0.210</td><td>0.153</td><td>0.118</td><td>0.096</td><td>0.081</td><td>0</td></tr>
  </tbody>
</table>
</p>
<p>
For the Taiwan stock index, we have \(n_{\nu}=15555\), sample mean \(\overline{x}=0.00015\), sample standard deviation \(s=0.00624\).
Setting the null hypothesis to \(\mu=0\), the naive t-statistic is \(\frac{\overline{x}-\mu}{\frac{s}{\sqrt{n_{\nu}}}}=3.01\), which is above the 5% critical value of 1.96.
However, since the Taiwan stock index has a tail index of \(\alpha=2.39\), the true gaussian degrees of freedom with a deviation that is equal to \(\text{Pareto}(2.39)\) is actually \(n_{g}=n_{\nu}^{\frac{2-2\kappa_{\alpha}(n_{\nu})}{2-\kappa_{\alpha}(n_{\nu})}}\).
From <a href="#table-1">Table 1</a>, we see that \(\kappa_{2.39}(15555)\simeq 0.231\).
This means the true equivalent gaussian sample size is \(n_{g}=15555^{\frac{2-2*0.231}{2-0.231}}=4411\), and the true t-statistic is 1.60, which is not significant.
Therefore, when using the Central Limit Theorem correctly, even with 58 years of data, it remains inconclusive whether the Taiwan stock market has a non-zero return.
</p>
<p>
For the common 80/20 law which is \(\text{Pareto}(\text{log}_{4}(5))=\text{Pareto}(1.16)\), to get mean deviations equivalent to a 30 sample gaussian, the Central Limit Theorem would require a sample size of \(30^{\frac{1}{1-\kappa_{\text{Pareto}(1.16)}(30)}}=30^{\frac{1}{1-0.863}}=6\cdot 10^{10}\).
</p>

<h2>Physics might dictate that the mean \(E(X)\) be non-existent</h2>
<p>Consider the intensity of a point light source on a vertical wall, as shown below:</p>
<div style="text-align: center;"><img src="light.svg"></img></div>
<p>The probability of \(\theta\) is the uniform distribution, and the distance between the light source and the wall is \(\ell\). We are interested in the distribution of \(x\). Since \(tan(\theta)=\frac{x}{\ell}\), the distribution is:</p>
$$\displaylines{
\begin{aligned}
P(X) =\space & 1 \cdot d\theta \\
=\space & \frac{d\theta}{dx}dx \\
=\space & \frac{1}{\pi} \frac{\ell}{\ell^{2}+x^{2}}dx
\end{aligned}
}$$
<p>which is the Cauchy distribution \(Cauchy(\ell)\). The Cauchy distribution has the property that the sum of two Cauchies is also a Cauchy, \(Cauchy(\ell_{1}) + Cauchy(\ell_{2}) = Cauchy(\ell_{1}+\ell_{2})\). This means we can view the distribution as a random walk by photons in small steps, known as the Huygens principle:</p>
<div style="text-align: center;"><img src="walk.svg"></div>
<p>We split \(\ell\) into \(n\) steps, and let \(X_{t}\) be the random variable describing a photon's random walk, \(P(X_{t}) = Cauchy(\frac{\ell}{n})\).
The law of large numbers says that the final location of the photon, \(X_{1} + X_{2} + ... + X_{n}\) approaches \(n\cdot E(X)\) as \(n\) increases.
In other words, we would observe a single spot of bright light at position \(n\cdot E(Cauchy(\frac{\ell}{n}))\), when the rest of the wall is completely dark.
However, a completely dark wall is completely nonsensical in physics.
The only way out of this paradox is to conclude that the mean of the Cauchy distribution does not exist.</p>

<hr>
<ol>
  <li id="foot_note-1">
    <b><a href="#foot_ref-1">^</a></b>
    We use 20 day returns for the Taiwan stock market, as opposed to the 1 day in Chapter 10 of Taleb<sup id="cite_ref-1e"><a href="#cite_note-1">[1e]</a></sup>.
    The reason is because Taiwan's 1 day return distribution is not a Pareto, due to the market regulation of capping daily stock price fluctuations at 7%.
  </li>
</ol>

<h2>Reference</h2>
<ol style="text-align: left;">
  <li id="cite_note-1">
    ^ <b><a href="#cite_ref-1a">a</a></b>
    <b><a href="#cite_ref-1b">b</a></b>
    <b><a href="#cite_ref-1c">c</a></b>
    <b><a href="#cite_ref-1d">d</a></b>
    <b><a href="#cite_ref-1e">e</a></b>
    Nassim Nicholas Taleb, Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications<a href="https://arxiv.org/abs/2001.10488">arXiv:2001.10488</a>
  </li>
</ol>

    </div>

    <script>
function onWindowResizeContainer() {
  const container = document.querySelector("#container");
  if (document.body.clientWidth/document.body.clientHeight > 1024/768) {
    container.style.width = "70%";
  } else {
    container.style.width = "100%";
  }
}

function main() {
  onWindowResizeContainer();
}
main();
    </script>

  </body>
</html>
